---
title       : OUTPUT
type        : residency
season      : Fall 2018

description : A performance project including dance, motion capture and choreographed robots including a 15 foot industrial robot nicknamed _Wen_, as a method of sourcing movement and character.

thumb-alt   : A woman facing forward looking at a phone with a large industrial robot behind her

image       : /images/projects/output/og.jpg

artist      : Catie Cuan

team :
  - name : Andrea Allen
    role : Developer
  - name : Felix Changoo
    role : Developer
  - name : Andrew McWilliams
    role : Lead Developer

extended-team :
  - name : Ellen Pearlman
    link : /bio/ellen-pearlman
  - name : Cole Belmont
  - name : Mark Parsons
    link : http://www.thatartist.com/
  - name : Gina Nikbin
  - name : Nour Sabs
  - name : Kira Davies
    link : /bio/kira-davies
  - name : Cameron Scoggins
---
Catie Cuan's _OUTPUT_ illuminates how bodies, both human and robotic, are mediated and represented through technology. It centers around a live performance which creates relationships between vestiges of real (human) and captured (technological) bodies.

{% include youtube id='Ng0ioS7oNXY' ratio='56'
   caption='Catie Cuan performing _OUTPUT_ at Triskelion Arts' %}

_OUTPUT_ premiered at Triskelion Arts’ Collaborations in Dance Festival in Brooklyn on September 14, 2018. This performance piece was aided by the development of two new software tools, _CONCAT_ and _MOSAIC_.

The _CONCAT_ system [visualizes a realtime 3D animation](https://github.com/thoughtworksarts/concat) of a 15 foot industrial robot arm, nicknamed _Wen_, moving alongside a live motion capture of a person. This shows the comparison and contrast of a human body next to an automated robot body, akin to a video game matching a live body to its mechanical shadow.

{% include image file='consortium.jpg'
   alt='Catie instructing the movement of a large industriual robot'
   caption='Catie working with a 15 foot industrial robot' %}

The _MOSAIC_ software used a webcam to generate time-delayed video segments, stacked side by side. The two softwares allow space for improvisation during live performance, creating a link between the human form and the robotic arm.

To create this performance, Catie collaborated with Thoughtworks developers and the [Consortium for Research and Robotics](https://consortiumrr.com/), a research center of Pratt Institute in New York.

{% include image file='andy-felix.jpg'
   alt='Engineers working in a robotics lab'
   caption='Thoughtworks developers working at the Consortium for Research &amp; Robotics' %}

OUTPUT was [featured in the New York Times](https://www.nytimes.com/2020/11/05/arts/dance/dance-and-artificial-intelligence.html), on [PBS NewsHour](/blog/concat-tool-feature-pbs/) in a segment on the future of work, and in an [Engadget video feature and article on robotic choreography](https://www.engadget.com/2018/10/12/robot-choreography-catie-cuan/).

The project was demonstrated at the TED Education Weekend in October, 2018, and exhibited at Pioneer Works Second Sunday in April, 2019, and at the Dance USA Conference in June, 2019.

{% include image file='concat.jpg'
   alt='A visitor trying out Catie\'s interactive exhibit'
   caption='CONCAT during a public showcase at Pioneer Works' %}

An academic paper describing the work and its implications [was published](http://aisb2019.machinemovementlab.net/MTSB2019_Cuan_Pearlman_McWilliams.pdf) in the _Movement that Shapes Behaviour_ symposium at the 2019 Artificial Intelligence and Simulation Behaviour Convention, titled _“OUTPUT: Translating Robot and Human Movers Across Platforms in a Sequentially Improvised Performance”_. The piece is written up in [Frontiers in Robotics and AI](https://www.frontiersin.org/articles/10.3389/frobt.2020.576790/full).