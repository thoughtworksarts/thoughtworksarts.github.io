<!DOCTYPE html>
<html lang=en>

<head>
  <title>Recognizing Human Facial Expressions With Machine Learning | Thoughtworks Arts</title>

  <meta charset=utf-8>
  <meta http-equiv=Content-Security-Policy content="
    default-src 'none';
    connect-src 'self' api.curator.io;
    base-uri    'self';
    img-src     'self'                 data:                                                cdn.cookielaw.org img.youtube.com curator-assets.b-cdn.net *.fbcdn.net *.twimg.com *.twitter.com;
    script-src  'self' 'unsafe-inline' 'unsafe-eval'                                        cdn.cookielaw.org code.jquery.com *.twitter.com *.google-analytics.com;
    style-src   'self' 'unsafe-inline' use.fontawesome.com cdn.curator.io fonts.gstatic.com cdn.cookielaw.org fonts.googleapis.com hello.myfonts.net;
    font-src    'self'                 use.fontawesome.com cdn.curator.io fonts.gstatic.com;
    frame-src   *.vimeo.com *.youtube.com *.twitter.com;
    form-action tinyletter.com">
  <meta http-equiv=X-UA-Compatible content="IE=edge">
  <meta name=viewport content="width=device-width, initial-scale=1.0">
  <meta name=Description content="Machine learning systems can be trained to recognize emotional expressions from images of human faces, with a high degree of accuracy in many cases.">

  <meta name=twitter:card content=summary_large_image>
  <meta name=twitter:site content=@tw_arts>
  <meta name=twitter:creator content=@tw_arts>
  <meta property=og:url content="https://thoughtworksarts.io/blog/recognizing-facial-expressions-machine-learning/">
  <meta property=og:title content="Recognizing Human Facial Expressions With Machine Learning | Thoughtworks Arts">
  <meta property=og:image content=https://thoughtworksarts.io/images/posts/2018-08-23-recognizing-facial-expressions-machine-learning/tsukiko-kiyomidzu.25a9.jpg>
  <meta property=og:description content="Machine learning systems can be trained to recognize emotional expressions from images of human faces, with a high degree of accuracy in many cases.">
  
  <meta property=article:published_time content=2018-08-23T12:00:00Z>
  

  <link rel=stylesheet href=https://use.fontawesome.com/releases/v5.2.0/css/all.css integrity=sha384-hWVjflwFxL6sNzntih27bfxkr27PmbbK/iSvJ+a4+0owXq79v+lsFkW54bOGbiDQ crossorigin=anonymous>
  <link href="https://fonts.googleapis.com/css?family=Open+Sans:400,700,400italic,700italic" rel=stylesheet type=text/css>
  <link href="https://fonts.googleapis.com/css?family=Oswald" rel=stylesheet>
  <link rel=stylesheet type=text/css href=//cdn.curator.io/3.1/css/curator.css>
  <link rel=canonical href="https://thoughtworksarts.io/blog/recognizing-facial-expressions-machine-learning/">
  <link rel=stylesheet href=/stylesheets/style.aa21.css>

  <!-- OneTrust Cookies Consent Notice start for thoughtworksarts.io -->
  <script src=https://cdn.cookielaw.org/consent/bd7c7510-ea19-4768-be84-506398c462f3.js type=text/javascript charset=UTF-8></script>
  <script type=text/javascript>
  function OptanonWrapper() { }
  </script>
<!-- OneTrust Cookies Consent Notice end for thoughtworksarts.io -->

  <!--[if lt IE 9]>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.2/html5shiv.min.js"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.2/html5shiv-printshiv.min.js"></script>
    <![endif]-->
  <link rel="shortcut icon" href=/images/logos/v2/favicon/favicon.ico>
</head>

<body id=news>
  <header role=banner>
    <section id=logonav>
      <h1>Thoughtworks Arts</h1>
      <a href="/" class=logo>
        <img src=/images/logos/v2/thoughtworks-arts.png alt="Thoughtworks Arts">
        <span class=strap>global technology research lab</span>
      </a>
      
      <div class=social-links>
  <h2>Social Media</h2>
  <ul>
    <li><a href=https://twitter.com/tw_arts aria-label=Twitter><i class="fab fa-twitter" aria-hidden=true></i></a></li>
    <li><a href="https://facebook.com/thoughtworksarts/" aria-label=Facebook><i class="fab fa-facebook-f" aria-hidden=true></i></a></li>
    <li><a href="https://github.com/thoughtworksarts/" aria-label=Github><i class="fab fa-github-alt" aria-hidden=true></i></a></li>
    <li><a href="https://medium.com/thoughtworksarts/" aria-label=Medium><i class="fab fa-medium-m" aria-hidden=true></i></a></li>
    <li><a href="https://instagram.com/thoughtworksarts/" aria-label=Instagram><i class="fab fa-instagram" aria-hidden=true></i></a></li>
  </ul>
</div>
      <nav class=navigation>
  <h2>Navigation</h2>
  <a id=menu-button href=# aria-hidden=true>
    <i class="fa fa-bars"></i>
    <i class="fa fa-times"></i>
  </a>
  <div class=overlay>
    <ul>
      <li><a href="/programs/" class=programs><span>Programs</span></a></li>
      <li><a href="/residents/" class=residents><span>Residents</span></a></li>
      <li><a href="/projects/" class=projects><span>Projects</span></a></li>
      <li><a href="/blog/" class=blog><span>News</span></a></li>
    </ul>
  </div>
</nav>
    </section>
    
  </header>
  <main>
  <section>
  <div class=content>
    <header>
      <h1>Blog</h1>
    </header>
    <article role=article>
      <header>
        <h1>Recognizing Human Facial Expressions With Machine Learning</h1>
        




<div class=widgets>
  <ul class="social-links new-window">
  <li>
    <a href="https://twitter.com/share?text=Recognizing%20Human%20Facial%20Expressions%20With%20Machine%20Learning&amp;url=https://thoughtworksarts.io/blog/recognizing-facial-expressions-machine-learning/&amp;via=tw_arts" aria-label=Twitter><i class="fab fa-twitter" aria-hidden=true></i></a>
  </li>
  <li>
    <a href="https://www.facebook.com/sharer/sharer.php?title=Recognizing%20Human%20Facial%20Expressions%20With%20Machine%20Learning&amp;u=https://thoughtworksarts.io/blog/recognizing-facial-expressions-machine-learning/" aria-label=Facebook><i class="fab fa-facebook" aria-hidden=true></i></a>
  </li>
  <li>
    <a href="https://www.tumblr.com/share?v=3&amp;t=Recognizing%20Human%20Facial%20Expressions%20With%20Machine%20Learning&amp;u=https://thoughtworksarts.io/blog/recognizing-facial-expressions-machine-learning/" aria-label=Tumblr><i class="fab fa-tumblr" aria-hidden=true></i></a>
  </li>
</ul>
  
    <figure>
      <a href="/bio/angelica-perez/"><img src=/images/people/angelica-perez.48cd.png alt="Angelica Perez"></a>
      <figcaption>Posted by <a href="/bio/angelica-perez/">Angelica Perez</a><br>Thursday, 23 August 2018</figcaption>
    </figure>
  
</div>
      </header>
      <div class=text>
        <p>Machine learning systems can be trained to recognize emotional expressions from images of human faces, with a high degree of accuracy in many cases.</p>

<figure>
	

		<img src=/images/posts/2018-08-23-recognizing-facial-expressions-machine-learning/tsukiko-kiyomidzu.25a9.jpg alt="A colorful painterly expression of a human face">
		

	

	
		<figcaption>Image by <a href="https://pixabay.com/en/man-people-girl-woman-women-girls-2013447/">Tsukiko Kiyomidzu</a></figcaption>
	
</figure>

<p>However, implementation can be a complex and difficult task. The technology is at a relatively early stage. High quality datasets can be hard to find. And there are various pitfalls to avoid when designing new systems.</p>

<!--excerpt-ends-->

<p>This article provides an introduction to the field known as Facial Expression Recognition (FER). After explaining general features and issues making up this field, this article will look at common FER datasets, architectures and algorithms. Further, it will examine the performance and accuracy of FER systems, showing how these outcomes are driving new trajectories for those exploring automated emotion recognition via machine learning.</p>

<h2 id=where-our-experience-came-from>Where our experience came from</h2>

<p>The information presented in this article is based on a mixture of project experience and academic research. At Thoughtworks we created an <a href=https://github.com/thoughtworksarts/EmoPy>FER toolkit named EmoPy</a>, supporting the artist and filmmaker Karen Palmer during her <a href="https://thoughtworksarts.io/blog/karen-palmer-ai-residency/">residency with Thoughtworks Arts</a>.</p>

<p>As the lead developer on that project, I worked with a team to help Karen to create a new version of her <a href="https://thoughtworksarts.io/projects/riot/">emotionally responsive film experience, RIOT</a>. We built EmoPy from the ground up to handle the emotion recognition requirements <a href=https://github.com/thoughtworksarts/riot>of the RIOT system</a>.</p>

<figure>
	

		<img src=/images/posts/2018-08-23-recognizing-facial-expressions-machine-learning/karen-palmer.777b.jpg alt="Karen Palmer pointing at a projection screen with lists of human emotions against numerical values">
		

	

	
		<figcaption>Karen Palmer using the EmoPy toolkit in her artwork</figcaption>
	
</figure>

<p>EmoPy is published as an open source project, helping to increase public access to a technology which is often locked behind closed doors. In <a href=/blog/emopy-emotional-expression-toolkit>this follow-up article</a> I explain more about the creation of the EmoPy toolkit, and explain why and how you might use it in your own projects.</p>

<p>Before joining Thoughtworks, I graduated from Stanford with a degree focusing on Artificial Intelligence. A large part of my research was in comparative approaches to machine learning problems, including looking at FER systems.</p>

<p>The overview presented in this article is drawn from both of these experiences, with Stanford and with Thoughtworks Arts.</p>

<h2 id=classifying-images-as-emotions>Classifying images as emotions</h2>

<p>Facial Expression Recognition is an <a href="http://cs231n.github.io/classification/">Image Classification</a> problem located within the wider field of Computer Vision. Image Classification problems are ones in which images must be algorithmically assigned a label from a discrete set of categories. In FER systems specifically, the images are of human faces and the categories are a set of emotions.</p>

<p>Machine learning approaches to FER all require a set of training image examples, each labeled with a single emotion category. A standard set of seven emotion classifications are often used:</p>

<ol>
  <li>Anger</li>
  <li>Disgust</li>
  <li>Fear</li>
  <li>Happiness</li>
  <li>Sadness</li>
  <li>Surprise</li>
  <li>Neutral</li>
</ol>

<p>These emotion classifications are illustrated in the image below, showing representative sample images taken from <a href=https://www.semanticscholar.org/paper/Facial-Expression-Recognition-Based-on-Facial-and-Chen-Chen/677ebde61ba3936b805357e27fce06c44513a455>this 2014 paper on expression recognition</a>.</p>

<p>Classifying an image based on it’s depiction can be a complicated task for machines. It is straightforward for humans to look at an image of a bicycle and know that it is a bicycle, or to look at a person’s face and know that they are smiling and happy.</p>

<figure class=no-border>
	

		<img src=/images/posts/2018-08-23-recognizing-facial-expressions-machine-learning/labeled-images.4b47.jpg alt="Eight greyscale human faces in a grid, labelled with text descriptions of emotional states">
		

	

	
		<figcaption>A selection of labeled images for expression analysis</figcaption>
	
</figure>

<p>When computers look at an image, what they ‘see’ is simply a matrix of pixel values. In order to classify an image, the computer has to discover and classify numerical patterns within the image matrix.</p>

<p>These patterns can be variable, and hard to pin down for multiple reasons. Several human emotions can be distinguished only by subtle differences in facial patterns, with emotions like anger and disgust often expressed in very similar ways. Each person’s expressions of emotions can be highly idiosyncratic, with particular quirks and facial cues. There can be a wide variety of divergent orientations and positions of people’s heads in the photographs to be classified.</p>

<p>For these types of reasons, FER is more difficult than most other Image Classification tasks. However, well-designed systems can achieve accurate results when constraints are taken into account during development.</p>

<p>For example, higher accuracy can be achieved when classifying a smaller subset of highly distinguishable expressions, such as anger, happiness and fear. Lower accuracy is achieved when classifying larger subsets, or small subsets with less distinguishable expressions, such as anger and disgust.</p>

<h2 id=common-expression-analysis-components>Common expression analysis components</h2>

<p>Like most image classification systems, FER systems typically use <em>image preprocessing</em> and <em>feature extraction</em> followed by training on selected <em>training architectures</em>. The end result of training is the generation of a <em>model</em> capable of assigning emotion categories to newly provided image examples.</p>

<figure class=no-border>
	

		<img src=/images/posts/2018-08-23-recognizing-facial-expressions-machine-learning/diagram.556d.png alt="Diagram showing system architecture">
		

	

	
		<figcaption>Commonly used FER system architectures</figcaption>
	
</figure>

<p>The <em>image preprocessing</em> stage can include image transformations such as scaling, cropping, or filtering images. It is often used to accentuate relevant image information, like cropping an image to remove a background. It can also be used to augment a dataset, for example to generate multiple versions from an original image with varying cropping or transformations applied.</p>

<p>The <em>feature extraction</em> stage goes further in finding the more descriptive parts of an image. Often this means finding information which can be most indicative of a particular class, such as the edges, textures, or colors.</p>

<p>The training stage takes place according to the defined <em>training architecture</em>, which determines the combinations of layers which feed into each other in the neural network. Architectures must be designed for training with the composition of the feature extraction and image preprocessing stages in mind. This is necessary because some architectural components work better with others when applied separately or together.</p>

<p>For example, certain types of feature extraction are not useful in conjunction with deep learning algorithms. They both find relevant features in images, such as edges, and therefore it is redundant to use the two together. Applying feature extraction prior to a deep learning algorithm is not only unnecessary, but can even negatively impact the performance of the architecture.</p>

<h2 id=a-comparison-of-training-algorithms>A comparison of training algorithms</h2>

<p>Once any feature extraction or image preprocessing stages are complete, the training algorithm produces a trained prediction model. A number of options exist for training FER models, each of which has strengths and weaknesses making them more or less suitable for particular situations.</p>

<p>In this article we will compare some of the most common algorithms used in FER:</p>

<ul>
  <li>Multiclass Support Vector Machines (SVM)</li>
  <li>Convolutional Neural Networks (CNN)</li>
  <li>Recurrent Neural Networks (RNN)</li>
  <li>Convolutional Long Short Term Memory (ConvLSTM)</li>
</ul>

<p>Multiclass <em>Support Vector Machines (SVM)</em> are <a href=http://www.cs.cmu.edu/~pmichel/publications/Michel-FacExpRecSVMPoster.pdf>supervised learning algorithms</a> that analyze and classify data, and they perform well when classifying human facial expressions. However, they only do so when the images are created in a controlled lab setting with consistent head poses and illumination.</p>

<p>SVMs perform less well when classifying images captured “in the wild,” or in spontaneous, uncontrolled settings. Therefore, the latest training architectures being explored are all deep neural networks which perform better under those circumstances. <em>Convolutional Neural Networks (CNN)</em> are currently considered the <a href=https://medium.com/technologymadeeasy/the-best-explanation-of-convolutional-neural-networks-on-the-internet-fbb8b1ad5df8>go-to neural networks for image classification</a>, because they pick up on patterns in small parts of an image, such as the curve of an eyebrow.</p>

<p>CNNs apply <a href=https://en.wikipedia.org/wiki/Kernel_(image_processing)>kernels</a>, which are matrices smaller than the image, to chunks of the input image. By applying kernels to inputs, new activation matrices, sometimes referred to as <a href=http://kaiminghe.com/iccv15tutorial/iccv2015_tutorial_convolutional_feature_maps_kaiminghe.pdf>feature maps</a>, are generated and passed as inputs to the next layer of the network. In this way, CNNs process more granular elements within an image, making them better at distinguishing between two similar emotion classifications.</p>

<p>Alternatively, <em>Recurrent Neural Networks (RNN)</em> use <a href=https://deeplearning4j.org/lstm.html#recurrent>dynamic temporal behavior</a> when classifying an image. This means that when an RNN processes an input example, it doesn’t just look at the data from that example — it also looks at the data from previous inputs, which are used to provide further context. In FER, the context could be previous image frames of a video clip.</p>

<p>The idea of this approach is to capture the <em>transitions between</em> facial patterns over time, allowing these changes to become additional data points supporting classification. For example, it is possible to capture the changes in the edges of the lips as an expression goes from neutral to happy by smiling, rather than just the edges of a smile from an individual image frame.</p>

<h2 id=combinations-for-greater-effect>Combinations for greater effect</h2>

<p>A CNN’s strength in extracting local data <a href="https://machinelearningmastery.com/cnn-long-short-term-memory-networks/">can be combined with</a> an RNN’s ability to use temporal context using <em>Convolutional Long Short Term Memory (ConvLSTM)</em>. These systems use convolutional layers to extract features and LSTM layers to capture changes in image sequences.</p>

<p>Since deep neural networks are good at identifying patterns in images, they can also be used for feature extraction. Some FER approaches use CNNs to produce feature vectors that are then sent to an SVM for classification. This approach can lead to more accurate results, but is a more complex architecture that requires extra programming effort and an increase in the processing time for each classified image.</p>

<p>The performance of any of these approaches varies depending on the input data, training parameters, emotion set and the system requirements. For these reasons it is important to experiment with various training architectures and datasets, assessing the accuracy and usefulness of each combination.</p>

<h2 id=finding-the-right-data>Finding the right data</h2>

<p>As described above, FER models must be trained on a set of labeled images before they can be used to classify new input images. Training for these applications requires large datasets of facial imagery with each displaying a discrete emotion — the more labeled images, the better.</p>

<p>Making a decision on which dataset to train the network on is no easy task, particularly because high quality FER datasets are hard to find. Few such datasets are publicly available, and those that are come with idiosyncrasies which must be understood and taken into account.</p>

<figure>
	

		<img src=/images/posts/2018-08-23-recognizing-facial-expressions-machine-learning/cohn-kanade.00d0.png alt="Five grayscale faces in a row, gradating between neutral and smiling facial expressions">
		

	

	
		<figcaption>Images from the Cohn-Kanade dataset (described below)</figcaption>
	
</figure>

<p>The most crucial points to consider when making a dataset selection are the <em>size</em> and <em>quality</em> of the set. The size is arguably the most important, and also the simplest to explain. Ideally a dataset should contain thousands, or preferably millions of images.</p>

<p>Many publicly-available FER datasets come with only hundreds, or sometimes thousands of images. By contrast, Affectiva’s <a href=http://blog.affectiva.com/the-worlds-largest-emotion-database-5.3-million-faces-and-counting>emotion database</a> of over 5 million faces are used for commercial emotion classification products. However, this data is not publicly available.</p>

<p>The quality of a dataset can be considered in various ways. One key consideration is the level of representation of given emotion classifications. It is common for certain emotions to be disproportionately under or over-represented by comparison to others in many datasets. Datasets generally contain far fewer examples of disgust, for example, than happiness or anger.</p>

<p>Another quality consideration is how the emotions expressed in datasets were stimulated. Were actors told to ‘play out’ emotions? Or, were people subjected to authentic emotional experiences, which were then recorded? These two approaches can lead to very different results.</p>

<p>A further consideration is that of head pose. What is the range of head poses, and orientations within the dataset? Crucially: how does that range relate to the range of head poses expected in your production environment?</p>

<p>What is the range of variability in terms of the illumination of the subject in each image, and how does that relate to your target environment? How were images labeled when the dataset was created and by whom? Is there any underlying bias in the labels that will increase the bias of your model?</p>

<p>All of these are questions of dataset quality which should be taken into account when selecting a dataset.</p>

<h2 id=datasets-and-target-emotions>Datasets and target emotions</h2>

<p>The selection of a dataset must be conducted with an eye to the set of target emotions for classification. As mentioned previously, some emotional expressions resemble each other. Also, subtle expressions, such as contempt, can be extremely hard to pick up on. Therefore, some datasets will outperform others for certain emotional sets.</p>

<p>Neural networks trained on a limited set of emotions generally tend to result in higher rates of accurate classifications. To give the most extreme example, training on a dataset containing only examples of happiness and anger tends to produce very high accuracy. Two such distinct target classifications mean that the overlap in facial expressive range is minimized.</p>

<p>When neural networks are trained on datasets containing under-represented emotions, such as disgust, they tend to misclassify those emotions. This occurs because multiple emotions can share similar facial features and the dataset does not contain large enough example sets of one emotion to find definitive distinctive patterns.</p>

<h2 id=transfer-learning-and-data-augmentation>Transfer learning and data augmentation</h2>

<p>All of these issues complicate the dataset selection process, and lead to the need for artificial improvement on whatever dataset you decide to use. This leads some to explore a technique called <a href="https://www.analyticsvidhya.com/blog/2017/06/transfer-learning-the-art-of-fine-tuning-a-pre-trained-model/">transfer learning</a>, an approach which takes knowledge gained while solving one problem and applies that knowledge to a different but related problem.</p>

<p>The idea behind transfer learning is to not start training from scratch. Instead, build on top of neural network models which have previously been trained on a large amount of <em>similar</em> data to solve some other similar problem. These models will already come with a set of pre-trained classification weights, which can be used as a starting point, and can be re-trained with a new dataset that uses a new set of classifications.</p>

<p>In the case of FER, neural nets can be used which were pre-trained on a large amount of images to solve general image classification problems, such as Google’s <a href=https://www.tensorflow.org/tutorials/image_recognition>Inception-V3</a> model. This model is trained on the <a href="http://image-net.org/">ImageNet dataset</a>, containing around 80,000 images of 1,000 classes, such as “zebra”, “dalmation,” and “dishwasher,” using a CNN.</p>

<p>However, there is a lot of room for improvement in transfer learning models for FER, as it is very difficult for such models to generalize across datasets. A model trained on one dataset can perform poorly when used to classify images from a second dataset.</p>

<p>Another approach is to use <a href=https://medium.com/nanonets/how-to-use-deep-learning-when-you-have-limited-data-part-2-data-augmentation-c26971dc8ced>data augmentation</a> to artificially extend the size of datasets. This approach creates copies of original images with altered illumination, applied rotations and mirroring, and other transformations. This approach can improve accuracy of neural networks by increasing the range of states in which source features are encountered during training.</p>

<h2 id=performance-in-practice>Performance in practice</h2>

<p>The tables below show the accuracy results for several recent deep neural net model architectures trained on two different publicly-available datasets:</p>

<ul>
  <li><a href=http://www.pitt.edu/~emotion/ck-spread.htm>Extended Cohn-Kanade</a>, released 2010, and</li>
  <li><a href=http://www.ee.oulu.fi/~gyzhao/Download/Databases/NIR_VL_FED/Description.pdf>Oulu-CASIA</a>, released 2009</li>
</ul>

<p>The tables come from <a href=https://arxiv.org/pdf/1707.04061.pdf>a 2017 academic research paper</a> that compares its own results, shown in the row labeled ‘Ours-Final’, to other architectures cited in the paper.</p>

<figure>
  <table>
    <tr>
      <th><strong>Method</strong></th>
      <th><strong>Accuracy</strong></th>
    </tr>
    <tr>
      <td>AURF [105]</td>
      <td>92.22 %</td>
    </tr>
    <tr>
      <td>AUDN [106]</td>
      <td>93.7 %</td>
    </tr>
    <tr>
      <td>STM-Explet [107]</td>
      <td>94.2 %</td>
    </tr>
    <tr>
      <td>LOmo [108]</td>
      <td>95.1 %</td>
    </tr>
    <tr>
      <td>IDT+FV [109]</td>
      <td>95.8 %</td>
    </tr>
    <tr>
      <td>Deep Belief Network [78]</td>
      <td>96.7 %</td>
    </tr>
    <tr>
      <td>Zero-Bias-CNN [110]</td>
      <td>98.4 %</td>
    </tr>
    <tr>
      <td>Ours-Final</td>
      <td>98.7 %</td>
    </tr>
  </table>
  <figcaption>Results of different deep neural net architectures trained on Cohn-Kanade dataset</figcaption>
</figure>

<p>The accuracy results reported in the first table are very high. However it is important to note that these models were trained and tested on the Cohn-Kanade dataset, which contains less than 500 image samples of facial expressions, and that these expressions were ‘acted out’ rather than authentically expressed.</p>

<p>Acted facial expressions are usually more exaggerated than what we see in the real world. If we were to classify new images with these models, they would most likely underperform. Systems trained on datasets created in a controlled lab setting generally fail to generalize across datasets.</p>

<figure>
  <table>
    <tr>
      <th><strong>Method</strong></th>
      <th><strong>Accuracy</strong></th>
    </tr>
    <tr>
      <td>DTAGN [111]</td>
      <td>81.46 %</td>
    </tr>
    <tr>
      <td>LOmo [108]</td>
      <td>82.1 %</td>
    </tr>
    <tr>
      <td>PPDN [112]</td>
      <td>84.59 %</td>
    </tr>
    <tr>
      <td>FN2EN [98]</td>
      <td>87.71 %</td>
    </tr>
    <tr>
      <td>Ours-Final</td>
      <td>89.60 %</td>
    </tr>
  </table>
  <figcaption>Results of different deep neural net architectures trained on Oulu-CASIA dataset</figcaption>
</figure>

<p>The Oulu-CASIA dataset is composed of six expressions: surprise, happiness, sadness, anger, fear and disgust. The images recorded are taken from a set of 80 people, the majority of which were male. Fifty of the subjects were Finnish, and thirty Chinese.</p>

<p>The expressions were not only acted out, but subjects were given facial expression images to use as examples to imitate. This is another example of a dataset with an extremely controlled creation process that results in lack of training diversity and a model that is unable to generalize.</p>

<h2 id=developing-new-projects-in-fer>Developing New Projects in FER</h2>

<p>These examples help to illustrate the primary issue faced when developing an FER system — generalizability. How can high accuracy be achieved across multiple datasets containing images with several variations of pose, illumination, and other distinguishing features?</p>

<p>These questions led us to develop <a href=https://github.com/thoughtworksarts/EmoPy>our own general FER toolkit</a>, drawing on the best available public data and utilizing the techniques above to produce accurate results. EmoPy is an open source system which is ready for general use.</p>

<p>When implementing new systems, it is necessary to look for weak spots and analyze the characteristics of your particular use-case. This can lead to unexpected solutions, such as when we recorded our own dataset closely matching the target production environment for the purposes of testing trained models.</p>

<p>We determined that the best way to achieve high accuracy <a href="https://thoughtworksarts.io/projects/riot/">in our project</a> was to train our model on a small dataset that matched the expected conditions of the art installation as closely as possible.</p>

<figure>
	<div class=video-wrapper>
	<div class=video>
		
			<iframe src=https://player.vimeo.com/video/275748144></iframe>
		
	</div>
</div>

	

	
		<figcaption>Dataset creation video by Karen Palmer</figcaption>
	
</figure>

<p>These conditions can be changeable as the experience is positioned differently in every exhibition of the work, but generally we expect dark rooms with lights shining on the faces of participants from a high angle. Also we expect participant’s facial expressions to be more subtle than ‘posed’, or acted-out expressions.</p>

<p>The size of our dataset was smaller than those that were publicly-available, but the quality was high because it was directed towards our needs.</p>

<h2 id=whats-next>What’s next</h2>

<p>This overview has outlined some of the broad topics and concerns of FER system development. My follow-up article explains <a href=/blog/emopy-emotional-expression-toolkit>more about the creation of the EmoPy toolkit</a>, and why and how you might use it in your own projects.</p>

      </div>
      <section id=calls-to-action>
        <p>Keep on top of Thoughtworks Arts updates and articles:</p>
        <ul>
          <li><a href="/newsletters/">Subscribe to our newsletter</a></li>
          <li><a href=https://twitter.com/tw_arts>Follow us on Twitter</a></li>
          <li><a href=https://medium.com/thoughtworks-arts>Follow us on Medium</a></li>
        </ul>
      </section>
    </article>
    <div class=progress>
      <a href="/blog/" class="blog forwards">All posts</a>
    </div>
  </div>
</section>
  </main>
  <footer>
    <div class=subscribe>
  <form class=newsletter-form action=https://tinyletter.com/thoughtworksarts method=post target=popupwindow onsubmit="window.open('https://tinyletter.com/thoughtworksarts', 'popupwindow', 'scrollbars=yes,width=800,height=600');return true">
    <label for=newsletter-subscribe-footer>Sign up for news by email:</label>
    <input type=email name=email id=newsletter-subscribe-footer value="your email" class="subscribe-newsletter faded">
    <input type=hidden value=1 name=embed>
    <input type=submit value=Subscribe>
  </form>
</div>
    <div class=social-links>
  <h2>Social Media</h2>
  <ul>
    <li><a href=https://twitter.com/tw_arts aria-label=Twitter><i class="fab fa-twitter" aria-hidden=true></i></a></li>
    <li><a href="https://facebook.com/thoughtworksarts/" aria-label=Facebook><i class="fab fa-facebook-f" aria-hidden=true></i></a></li>
    <li><a href="https://github.com/thoughtworksarts/" aria-label=Github><i class="fab fa-github-alt" aria-hidden=true></i></a></li>
    <li><a href="https://medium.com/thoughtworksarts/" aria-label=Medium><i class="fab fa-medium-m" aria-hidden=true></i></a></li>
    <li><a href="https://instagram.com/thoughtworksarts/" aria-label=Instagram><i class="fab fa-instagram" aria-hidden=true></i></a></li>
  </ul>
</div>
    <ul class=notices>
  <li>
    <a href="/newsletters/">Newsletter Archive</a>
  </li>
  <li>
    <a href="https://www.thoughtworks.com/privacy-policy/">Privacy Policy</a>
  </li>
</ul>
  </footer>
  <script src=/javascript/script.51ea.js></script>
</body>
</html>